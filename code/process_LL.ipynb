{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process nodes files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data for file: SA_G_20230605_112252.tsv\n",
      "Processed data for file: SA_G_20230612_092113.tsv\n",
      "Processed data for file: SA_G_20230619_073005.tsv\n",
      "Processed data for file: SA_G_20230626_064142.tsv\n",
      "Processed data for file: SA_G_20230703_071341.tsv\n",
      "Processed data for file: SA_G_20230710_084845.tsv\n",
      "Processed data for file: SA_G_20230717_065818.tsv\n",
      "Processed data for file: SA_G_20230724_064216.tsv\n",
      "Processed data for file: SA_G_20230731_075054.tsv\n",
      "Processed data for file: SA_G_20230807_064840.tsv\n",
      "Processed data for file: SA_G_20230814_074003.tsv\n",
      "Processed data for file: SA_G_20230820_173845.tsv\n",
      "Processed data for file: SA_G_20230827_164308.tsv\n",
      "Processed data for file: SA_G_20230903_145652.tsv\n",
      "Processed data for file: SA_G_20230910_145957.tsv\n"
     ]
    }
   ],
   "source": [
    "def process_tsv_file(filepath):\n",
    "    df = pd.read_csv(filepath, sep='\\t', header=None, skiprows=36)\n",
    "    return df\n",
    "\n",
    "# Define a dictionary with node IDs as keys and lists of column names as values\n",
    "column_names = {\n",
    "    '19F798E': ['Node_ID', 'DateTime', 'Status', 'Bi1', 'Connection', 'Temp_Air(C)', 'Illu(lx)', 'Ele1(A)', 'Ele2(A)', 'Bi2', 'Door', 'Window'],  \n",
    "    '19F7993': ['Node_ID', 'DateTime', 'Status', 'Bi1', 'Connection', 'Temp_Air(C)', 'Illu(lx)', 'Ele1(A)', 'Ele2(A)', 'Bi2', 'Window', 'Door'],  \n",
    "    '1A057E0': ['Node_ID', 'DateTime', 'Status', 'Bi1', 'Connection', 'Temp_Air(C)', 'Illu(lx)', 'Ele1(A)', 'Ele2(A)', 'Bi2', 'Door', 'Window'],\n",
    "    '1A057E5': ['Node_ID', 'DateTime', 'Status', 'Bi1', 'Connection', 'Temp_Air(C)', 'Illu(lx)', 'Ele1(A)', 'Ele2(A)', 'Bi2', 'Door', 'Window'],\n",
    "    '1A00DBE': ['Node_ID', 'DateTime', 'Status', 'Bi1', 'Connection', 'Temp_Air(C)', 'Illu(lx)', 'Ele1(A)', 'Ele2(A)', 'Bi2', 'Window', 'Door'], \n",
    "    '1A00DC1': ['Node_ID', 'DateTime', 'Status', 'Bi1', 'Connection', 'CO2', 'Temp_Air(C)', 'RH(%)', 'Temp_Globe(C)', 'Bi2', 'x', 'x'], \n",
    "    '19FD06D': ['Node_ID', 'DateTime', 'Status', 'Bi1', 'Connection', 'CO2', 'Temp_Air(C)', 'RH(%)', 'Temp_Globe(C)', 'Bi2', 'x', 'x'], \n",
    "    '1A057DF': ['Node_ID', 'DateTime', 'Status', 'Bi1', 'Connection', 'CO2', 'Temp_Air(C)', 'RH(%)', 'Temp_Globe(C)', 'Bi2', 'x', 'x'] \n",
    "    }\n",
    "\n",
    "nodes = {'19F798E', '19F7993', '19FD06D', '1A057E5', '1A00DC1', '1A00DBE', '1A057DF', '1A057E0'}\n",
    "\n",
    "basepath = 'C:/Users/Tomar/dev/datasets/living_lab_2023/'  \n",
    "outpath = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/'\n",
    "\n",
    "def node_data(basepath, nodes, outpath, column_names):\n",
    "    node_data = {node: [] for node in nodes}\n",
    "    tsv_files = [file for file in os.listdir(basepath) if file.endswith('.tsv')]\n",
    "    \n",
    "    for tsv_file in tsv_files:\n",
    "        filepath = os.path.join(basepath, tsv_file)\n",
    "        df = process_tsv_file(filepath)\n",
    "        print(f\"Processed data for file: {tsv_file}\")\n",
    "        \n",
    "        for node in nodes:\n",
    "            node_df = df[df[0] == node]  # Assuming the first column is the Node_ID\n",
    "            if node in column_names:\n",
    "                node_df.columns = column_names[node]  # Apply node-specific column names\n",
    "                # Move DateTime column to the first position\n",
    "                cols = node_df.columns.tolist()\n",
    "                cols = [cols[1]] + cols[0:1] + cols[2:]\n",
    "                node_df = node_df[cols]\n",
    "                \n",
    "            node_data[node].append(node_df)\n",
    "    \n",
    "    for node, dfs in node_data.items():\n",
    "        if not dfs:\n",
    "            continue\n",
    "        node_df = pd.concat(dfs, ignore_index=True)\n",
    "        node_filepath = os.path.join(outpath, f\"node_{node}.csv\")\n",
    "        node_df.to_csv(node_filepath, index=False)\n",
    "\n",
    "# Call the function with the node-column mapping\n",
    "node_data(basepath, nodes, outpath, column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process S02, S06, S08 subjects file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_row(input_row):\n",
    "    parts = input_row.split(',')\n",
    "    corrected_parts = [parts[0]]  \n",
    "    i = 1\n",
    "    while i < len(parts):\n",
    "        if parts[i] == \"-999999\" or parts[i] == \"0\":\n",
    "            corrected_parts.append(parts[i])\n",
    "            i += 1  \n",
    "        elif i + 1 < len(parts) and parts[i+1].isdigit():\n",
    "            decimal_number = f\"{parts[i]}.{parts[i+1]}\"\n",
    "            corrected_parts.append(decimal_number)\n",
    "            i += 2  \n",
    "        else:\n",
    "            corrected_parts.append(parts[i])\n",
    "            i += 1  \n",
    "    output_row = ','.join(corrected_parts)\n",
    "    return output_row\n",
    "\n",
    "def transform_file(input_file_path, output_file_path):\n",
    "    with open(input_file_path, 'r') as input_file, open(output_file_path, 'w') as output_file:\n",
    "        for line in input_file:\n",
    "            transformed_line = transform_row(line.strip())\n",
    "            output_file.write(transformed_line + '\\n')\n",
    "\n",
    "def process_files(input_files, output_folder):\n",
    "    for input_file_path in input_files:\n",
    "        # Construct output file path\n",
    "        file_name = os.path.basename(input_file_path)\n",
    "        output_file_path = os.path.join(output_folder, f\"{os.path.splitext(file_name)[0]}.csv\")\n",
    "        \n",
    "        # Transform the file\n",
    "        transform_file(input_file_path, output_file_path)\n",
    "        \n",
    "        # Read the transformed CSV file into DataFrame\n",
    "        df = pd.read_csv(output_file_path)\n",
    "        \n",
    "        # Drop specified columns\n",
    "        columns_to_drop = [6, 12, 14]\n",
    "        df.drop(df.columns[columns_to_drop], axis=1, inplace=True)\n",
    "        \n",
    "        # Rename columns\n",
    "        column_names = ['DateTime', 'Temp_Globe(C)', 'TNW', 'Temp_Air(C)', 'RH(%)', 'Air_Velocity(m/s)', 'WBGT(C)', 'WBGTsi(C)', 'HI(C)', 'HX(C)', 'PMV(C)', 'PPD']\n",
    "        df.columns = column_names\n",
    "        \n",
    "        # Write the processed DataFrame to CSV\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# Specify input and output folder paths\n",
    "input_folder = 'C:/Users/Tomar/dev/datasets/living_lab_2023'\n",
    "output_folder = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab'\n",
    "\n",
    "# List input files\n",
    "input_files = [\n",
    "    os.path.join(input_folder, 'Luca.txt'),\n",
    "    os.path.join(input_folder, 'Carolina.txt'),\n",
    "    os.path.join(input_folder, 'Silvia.txt')\n",
    "]\n",
    "\n",
    "# Process files\n",
    "process_files(input_files, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct DateTime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Carolina.csv\n",
      "Processed Luca.csv\n",
      "Processed node_19F798E.csv\n",
      "Processed node_19F7993.csv\n",
      "Processed node_19FD06D.csv\n",
      "Processed node_1A00DBE.csv\n",
      "Processed node_1A00DC1.csv\n",
      "Processed node_1A057DF.csv\n",
      "Processed node_1A057E0.csv\n",
      "Processed node_1A057E5.csv\n",
      "Processed Silvia.csv\n"
     ]
    }
   ],
   "source": [
    "def try_parsing_date(text):\n",
    "    \"\"\"\n",
    "    Try to parse the datetime string using multiple formats.\n",
    "    \"\"\"\n",
    "    for fmt in (\"%Y-%m-%dT%H:%M:%S\", \"%d/%m/%Y %H:%M:%S\", \"%Y-%m-%d %H:%M:%S\"):  # Add more formats here as needed\n",
    "        try:\n",
    "            return pd.to_datetime(text, format=fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    raise ValueError(\"no valid date format found\")\n",
    "\n",
    "def convert_all_files_in_folder(folder_path, output_folder):\n",
    "    \"\"\"\n",
    "    Convert datetime format for all files in a given folder.\n",
    "    \"\"\"\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):  \n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            output_file_path = os.path.join(output_folder, filename)\n",
    "\n",
    "            # Load the file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Attempt to convert the DateTime column\n",
    "            df['DateTime'] = df['DateTime'].apply(try_parsing_date)\n",
    "            df['DateTime'] = df['DateTime'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "            # Save the converted file\n",
    "            df.to_csv(output_file_path, index=False)\n",
    "\n",
    "            print(f\"Processed {filename}\")\n",
    "\n",
    "# Specify the folder containing the original files and the folder to save the converted files\n",
    "folder_path = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/'\n",
    "output_folder = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/LL/'\n",
    "convert_all_files_in_folder(folder_path, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes data into LL and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "## Living lab 15\n",
    "\n",
    "file_path0 = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/final/node_19F798E.csv'\n",
    "\n",
    "# Load the data\n",
    "data0 = pd.read_csv(file_path0)\n",
    "\n",
    "    \n",
    "# Assuming 'DateTime' is present in both datasets, convert them to datetime type for proper merging\n",
    "data0['DateTime'] = pd.to_datetime(data0['DateTime'])\n",
    "    \n",
    "\n",
    "# Selecting only the required columns to create a new DataFrame as specified\n",
    "required_columns = ['DateTime', 'Temp_Air(C)', 'Illu(lx)', 'Ele1(A)', 'Ele2(A)', 'Window', 'Door' ]\n",
    "LL15 = data0[required_columns]\n",
    "\n",
    "\n",
    "path0 = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/LL/LL15.csv'\n",
    "LL15.to_csv(path0, index=False)\n",
    "\n",
    "\n",
    "###############################################################\n",
    "## Living lab 16\n",
    "\n",
    "file_path1 = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/final/node_19F7993.csv'\n",
    "file_path2 = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/final/node_19FD06D.csv'\n",
    "\n",
    "# Load the data\n",
    "data1 = pd.read_csv(file_path1)\n",
    "data2 = pd.read_csv(file_path2)\n",
    "    \n",
    "# Assuming 'DateTime' is present in both datasets, convert them to datetime type for proper merging\n",
    "data1['DateTime'] = pd.to_datetime(data1['DateTime'])\n",
    "data2['DateTime'] = pd.to_datetime(data2['DateTime'])\n",
    "    \n",
    "# Merge the datasets on 'DateTime'\n",
    "combined_LL16 = pd.merge(data1, data2, on='DateTime', how='outer')\n",
    "\n",
    "# Selecting only the required columns to create a new DataFrame as specified\n",
    "required_columns = ['DateTime', 'CO2', 'RH(%)', 'Temp_Globe(C)', 'Temp_Air(C)_x', 'Illu(lx)', 'Ele1(A)', 'Ele2(A)', 'Window', 'Door' ]\n",
    "LL16 = combined_LL16[required_columns]\n",
    "\n",
    "path1 = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/LL/LL16.csv'\n",
    "LL16.to_csv(path1, index=False)\n",
    "\n",
    "\n",
    "#############################################################\n",
    "## Living Lab 17\n",
    "\n",
    "file_path3 = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/final/node_1A057E5.csv'\n",
    "file_path4 = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/final/node_1A00DC1.csv'\n",
    "\n",
    "# Load the data\n",
    "data3 = pd.read_csv(file_path3)\n",
    "data4 = pd.read_csv(file_path4)\n",
    "    \n",
    "# Assuming 'DateTime' is present in both datasets, convert them to datetime type for proper merging\n",
    "data3['DateTime'] = pd.to_datetime(data3['DateTime'])\n",
    "data4['DateTime'] = pd.to_datetime(data4['DateTime'])\n",
    "    \n",
    "# Merge the datasets on 'DateTime'\n",
    "combined_LL17 = pd.merge(data3, data4, on='DateTime', how='outer')\n",
    "\n",
    "# Selecting only the required columns to create a new DataFrame as specified\n",
    "required_columns = ['DateTime', 'CO2', 'RH(%)', 'Temp_Globe(C)', 'Temp_Air(C)_x', 'Illu(lx)', 'Ele1(A)', 'Ele2(A)', 'Window', 'Door' ]\n",
    "LL17 = combined_LL17[required_columns]\n",
    "\n",
    "\n",
    "path2 = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/LL/LL17.csv'\n",
    "LL17.to_csv(path2, index=False)\n",
    "\n",
    "\n",
    "##################################################################\n",
    "## Living Lab 18\n",
    "\n",
    "file_path5 = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/final/node_1A00DBE.csv'\n",
    "file_path6 = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/final/node_1A057DF.csv'\n",
    "\n",
    "# Load the data\n",
    "data5 = pd.read_csv(file_path5)\n",
    "data6 = pd.read_csv(file_path6)\n",
    "    \n",
    "# Assuming 'DateTime' is present in both datasets, convert them to datetime type for proper merging\n",
    "data5['DateTime'] = pd.to_datetime(data5['DateTime'])\n",
    "data6['DateTime'] = pd.to_datetime(data6['DateTime'])\n",
    "    \n",
    "# Merge the datasets on 'DateTime'\n",
    "combined_LL18 = pd.merge(data5, data6, on='DateTime', how='outer')\n",
    "\n",
    "# Selecting only the required columns to create a new DataFrame as specified\n",
    "required_columns = ['DateTime', 'CO2', 'RH(%)', 'Temp_Globe(C)', 'Temp_Air(C)_x', 'Illu(lx)', 'Ele1(A)', 'Ele2(A)', 'Window', 'Door' ]\n",
    "LL18 = combined_LL18[required_columns]\n",
    "\n",
    "path3 = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/LL/LL18.csv'\n",
    "LL18.to_csv(path3, index=False)\n",
    "\n",
    "\n",
    "###############################################################\n",
    "## Living Lab 19\n",
    "\n",
    "file_path7 = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/final/node_1A057E0.csv'\n",
    "\n",
    "# Load the data\n",
    "data7 = pd.read_csv(file_path7)\n",
    "    \n",
    "# Assuming 'DateTime' is present in both datasets, convert them to datetime type for proper merging\n",
    "data7['DateTime'] = pd.to_datetime(data7['DateTime'])\n",
    "\n",
    "# Selecting only the required columns to create a new DataFrame as specified\n",
    "required_columns = ['DateTime', 'Temp_Air(C)', 'Illu(lx)', 'Ele1(A)', 'Ele2(A)', 'Window', 'Door' ]\n",
    "LL19 = data7[required_columns]\n",
    "\n",
    "path4 = 'C:/Users/Tomar/dev/vehicle_indoor_comfort/summer_2023/output/living_lab/LL/LL19.csv'\n",
    "LL19.to_csv(path4, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
