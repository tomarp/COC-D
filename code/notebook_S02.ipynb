{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant ID:S02\n",
    "\n",
    "### Content:\n",
    "- Correlation matrix \n",
    "- Comfort Function implementation\n",
    "- Weather Station Data (During Commuting)\n",
    "- Weather Station Data (Day of Commuting from 8am to 6pm)\n",
    "- Living Lab (Day of Commuting from 8am to 6pm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the CSV file into a DataFrame\n",
    "S02_path = 'C:/Users/Tomar/dev/PCM_study/summer_2023/output/process_data/S02_final_df.csv'\n",
    "\n",
    "S02_df = pd.read_csv(S02_path, index_col=False, parse_dates=['DateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S02_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'DateTime' column as the index\n",
    "S02_df.set_index('DateTime', inplace=True)\n",
    "\n",
    "# Group by the unique ID instances\n",
    "grouped = S02_df.groupby('ID_instance')\n",
    "\n",
    "# Prepare a list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate through each group\n",
    "for name, group in grouped:\n",
    "    # Sort the group by DateTime if not already sorted\n",
    "    group = group.sort_index()\n",
    "    \n",
    "    # Calculate the duration\n",
    "    start_time = group.index[0]\n",
    "    end_time = group.index[-1]\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    # Calculate the min, max, and avg for Temp(C)_N1 and RH(%)_N1\n",
    "    temp_n1_min = group['Temp(C)_N1'].min()\n",
    "    temp_n1_max = group['Temp(C)_N1'].max()\n",
    "    temp_n1_avg = group['Temp(C)_N1'].mean()\n",
    "    \n",
    "    rh_n1_min = group['RH(%)_N1'].min()\n",
    "    rh_n1_max = group['RH(%)_N1'].max()\n",
    "    rh_n1_avg = group['RH(%)_N1'].mean()\n",
    "    \n",
    "    # Append the result to the list\n",
    "    results.append([name, start_time, end_time, duration, \n",
    "                    temp_n1_min, temp_n1_max, temp_n1_avg, \n",
    "                    rh_n1_min, rh_n1_max, rh_n1_avg])\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "results_df = pd.DataFrame(results, columns=['ID_instance', 'Start Time', 'End Time', 'Duration', \n",
    "                                            'Temp(C)_N1_min', 'Temp(C)_N1_max', 'Temp(C)_N1_avg',\n",
    "                                            'RH(%)_N1_min', 'RH(%)_N1_max', 'RH(%)_N1_avg'])\n",
    "\n",
    "# Print the results as a table\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'id_instance' and calculate time duration\n",
    "time_duration = S02_df.groupby('ID_instance').apply(lambda x: x.index.max() - x.index.min())\n",
    "\n",
    "# Convert time duration to seconds for better readability (optional)\n",
    "time_duration = (time_duration.dt.total_seconds()/60).round(2)\n",
    "# time_duration = time_duration.round(2)\n",
    "\n",
    "print(time_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances_over_60_minutes = time_duration[time_duration > 60].index\n",
    "\n",
    "# Remove rows corresponding to instances over 60 minutes inplace\n",
    "S02_df.drop(S02_df[S02_df['ID_instance'].isin(instances_over_60_minutes)].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S02_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S02_df.dropna(subset=['ID_instance'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S02_df['ID_instance'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S02_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns starting with 'acc_'\n",
    "S02_df = S02_df.filter(regex='^(?!acc_)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S02_df.columns[1:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S02_df.columns[18:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting physiological features (excluding Subject and Commuting columns)\n",
    "physiological_data = S02_df.columns[1:18] \n",
    "\n",
    "# Selecting environmental parameters\n",
    "environmental_data = S02_df.columns[18:30] \n",
    "\n",
    "# Calculating correlation matrix\n",
    "correlation_matrix = S02_df[list(physiological_data) + list(environmental_data)].corr(method='spearman')\n",
    "\n",
    "# Extracting correlations between physiological features and environmental parameters\n",
    "correlation_phys_env = correlation_matrix.loc[physiological_data, environmental_data]\n",
    "\n",
    "correlation_phys_env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new column names with LaTeX subscript representation\n",
    "new_column_names = {\n",
    "    'Temp(C)_N1': 'T$_{1}$',\n",
    "    'RH(%)_N1': 'H$_{1}$',\n",
    "    'Dewpoint(C)_N1': 'D$_{1}$',\n",
    "    'Temp(C)_N2': 'T$_{2}$',\n",
    "    'RH(%)_N2': 'H$_{2}$',\n",
    "    'Dewpoint(C)_N2': 'D$_{2}$',\n",
    "    'Temp(C)_N3': 'T$_{3}$',\n",
    "    'RH(%)_N3': 'H$_{3}$',\n",
    "    'Dewpoint(C)_N3': 'D$_{3}$',\n",
    "    'Temp(C)_N4': 'T$_{4}$',\n",
    "    'RH(%)_N4': 'H$_{4}$',\n",
    "    'Dewpoint(C)_N4': 'D$_{4}$',\n",
    "\n",
    "    'hrv_vlf': 'HRV$_1$',\n",
    "    'hrv_lf': 'HRV$_2$',\n",
    "    'hrv_hf': 'HRV$_3$',\n",
    "    'hrv_sdnn': 'HRV$_4$',\n",
    "    'hrv_rmssd': 'HRV$_5$',\n",
    "    'hrv_mean_nni': 'HRV$_6$',\n",
    "    'hrv_median_nni': 'HRV$_7$',\n",
    "    'hrv_range_nni': 'HRV$_8$',\n",
    "    'hrv_entropy': 'HRV$_9$',\n",
    "\n",
    "    'eda_tonic_mean': 'EDA$_1$',\n",
    "    'eda_tonic_std': 'EDA$_2$',\n",
    "    'eda_tonic_sum': 'EDA$_3$',\n",
    "    'eda_tonic_energy': 'EDA$_4$',\n",
    "    'eda_phasic_mean': 'EDA$_5$',\n",
    "    'eda_phasic_std': 'EDA$_6$',\n",
    "    'eda_phasic_sum': 'EDA$_7$',\n",
    "    'eda_phasic_energy': 'EDA$_8$',\n",
    "\n",
    "    # 'acc_acc_x_min': 'ACC$_1$',\n",
    "    # 'acc_acc_y_min': 'ACC$_2$' ,\n",
    "    # 'acc_acc_z_min': 'ACC$_3$', \n",
    "    # 'acc_l2_min': 'ACC$_4$',\n",
    "    # 'acc_acc_x_max': 'ACC$_5$',\n",
    "    # 'acc_acc_y_max': 'ACC$_6$', \n",
    "    # 'acc_acc_z_max': 'ACC$_7$', \n",
    "    # 'acc_l2_max': 'ACC$_8$',\n",
    "    # 'acc_acc_x_ptp': 'ACC$_9$', \n",
    "    # 'acc_acc_y_ptp': 'ACC$_{10}$', \n",
    "    # 'acc_acc_z_ptp': 'ACC$_{11}$', \n",
    "    # 'acc_l2_ptp': 'ACC$_{12}$'\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "correlation_phys_env_renamed = correlation_phys_env.rename(columns=new_column_names, index=new_column_names)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Generate a heatmap of the correlation matrix\n",
    "sns.heatmap(correlation_phys_env_renamed, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "\n",
    "# Add titles and labels\n",
    "# plt.title('S02')\n",
    "# plt.xlabel('environmental parameters', fontsize=14)\n",
    "# plt.ylabel('physiological features', fontsize=14)\n",
    "\n",
    "# Rotate the y-axis labels for better readability\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=0)\n",
    "\n",
    "# Save the plot as eps & png file\n",
    "outpath = 'C:/Users/Tomar/dev/PCM_study/summer_2023/output/viz/'\n",
    "plt.savefig(outpath+'S02_heatmap.eps', format='eps', bbox_inches='tight')\n",
    "plt.savefig(outpath+'S02_heatmap.png', format='png', bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comfort function Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all physiological variables\n",
    "physio_vars = ['hrv_vlf', 'hrv_lf', 'hrv_hf', 'hrv_sdnn', 'hrv_rmssd',\n",
    "               'hrv_mean_nni', 'hrv_median_nni', 'hrv_range_nni', 'hrv_entropy',\n",
    "               'eda_tonic_mean', 'eda_tonic_std', 'eda_tonic_sum', 'eda_tonic_energy',\n",
    "               'eda_phasic_mean', 'eda_phasic_std', 'eda_phasic_sum', 'eda_phasic_energy']\n",
    "\n",
    "# Environmental variables to consider\n",
    "env_vars = ['Temp(C)_N1', 'RH(%)_N1']\n",
    "\n",
    "# Compute Spearman correlations\n",
    "correlations = S02_df[physio_vars + env_vars].corr(method='spearman')\n",
    "\n",
    "# HRV and EDA groups for correlation\n",
    "hrv_vars = physio_vars[:9]  # Assuming the first 9 are HRV related\n",
    "eda_vars = physio_vars[9:]  # Assuming the rest are EDA related\n",
    "\n",
    "# Identify one HRV and one EDA variable with the strongest correlation with any environmental variable\n",
    "strongest_corr_hrv = correlations.loc[hrv_vars, env_vars].abs().max().idxmax()\n",
    "strongest_corr_eda = correlations.loc[eda_vars, env_vars].abs().max().idxmax()\n",
    "\n",
    "# Extract the specific HRV and EDA variable names with the highest correlation\n",
    "strongest_corr_hrv_var = correlations.loc[hrv_vars, env_vars].abs().idxmax()[strongest_corr_hrv]\n",
    "strongest_corr_eda_var = correlations.loc[eda_vars, env_vars].abs().idxmax()[strongest_corr_eda]\n",
    "\n",
    "# Define thresholds using the identified variables and adjusted quartiles for environmental variables\n",
    "thresholds = {\n",
    "    strongest_corr_hrv_var: S02_df[strongest_corr_hrv_var].quantile(0.75),\n",
    "    strongest_corr_eda_var: S02_df[strongest_corr_eda_var].quantile(0.25),\n",
    "    'Temp(C)_N1': [S02_df['Temp(C)_N1'].quantile(0.30), S02_df['Temp(C)_N1'].quantile(0.65)],\n",
    "    'RH(%)_N1': [S02_df['RH(%)_N1'].quantile(0.30), S02_df['RH(%)_N1'].quantile(0.65)]\n",
    "}\n",
    "\n",
    "# Define the comfort function\n",
    "def comfort_label(row):\n",
    "    comfort = (row[strongest_corr_hrv_var] >= thresholds[strongest_corr_hrv_var] and\n",
    "               row[strongest_corr_eda_var] <= thresholds[strongest_corr_eda_var] and\n",
    "               thresholds['Temp(C)_N1'][0] <= row['Temp(C)_N1'] <= thresholds['Temp(C)_N1'][1] and\n",
    "               thresholds['RH(%)_N1'][0] <= row['RH(%)_N1'] <= thresholds['RH(%)_N1'][1])\n",
    "    \n",
    "    if comfort:\n",
    "        return 1\n",
    "    \n",
    "    discomfort = (row[strongest_corr_hrv_var] < thresholds[strongest_corr_hrv_var] and\n",
    "                  row[strongest_corr_eda_var] > thresholds[strongest_corr_eda_var] and\n",
    "                  not (thresholds['Temp(C)_N1'][0] <= row['Temp(C)_N1'] <= thresholds['Temp(C)_N1'][1]) or\n",
    "                  not (thresholds['RH(%)_N1'][0] <= row['RH(%)_N1'] <= thresholds['RH(%)_N1'][1]))\n",
    "    \n",
    "    if discomfort:\n",
    "        return 0\n",
    "    \n",
    "    return 0.5  # Sensible label for ambiguous cases\n",
    "\n",
    "# Apply the comfort function and count labels\n",
    "S02_df['lambda'] = S02_df.apply(comfort_label, axis=1)\n",
    "label_counts = S02_df['lambda'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Print the names of variables used for the comfort function\n",
    "print(\"Variables considered for the comfort function:\")\n",
    "print(f\"HRV Variable: {strongest_corr_hrv_var}\")\n",
    "print(f\"EDA Variable: {strongest_corr_eda_var}\")\n",
    "print(\"Environmental Variables: Temp(C)_N1, RH(%)_N1\")\n",
    "\n",
    "# Print the percentage of labeled values\n",
    "print(\"Label Distribution (%):\")\n",
    "print(label_counts)\n",
    "\n",
    "print(\"saving data files as pkl\")\n",
    "savepath = 'C:/Users/Tomar/dev/PCM_study/summer_2023/output/final_data/data_S02.pkl'\n",
    "S02_df.to_pickle(savepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by unique instances\n",
    "instance_groups = S02_df.groupby('ID_instance')\n",
    "\n",
    "# Calculate the level of comfort for each instance\n",
    "comfort_levels = instance_groups['lambda'].mean()  # You can also use .median() or .mode() depending on your preference\n",
    "\n",
    "# Print the comfort levels\n",
    "print(comfort_levels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Station Data (During Commuting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the weather data from the uploaded Excel file\n",
    "file_path = 'C:/Users/Tomar/dev/datasets/weather_summer2023.xlsx'\n",
    "weather_data = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows of the weather data to understand its structure\n",
    "weather_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the commuting instance data\n",
    "commuting_data = [\n",
    "    {\"ID_instance\": \"S02_1\", \"Start Time\" : \"2023-07-31 16:33:54\", \"End Time\": \"2023-07-31 16:36:38\"},\n",
    "    {\"ID_instance\": \"S02_2\", \"Start Time\" : \"2023-08-01 16:38:07\", \"End Time\": \"2023-08-01 16:54:32\"},\n",
    "    {\"ID_instance\": \"S02_3\", \"Start Time\" : \"2023-08-01 20:34:40\", \"End Time\": \"2023-08-01 20:45:29\"},\n",
    "    {\"ID_instance\": \"S02_4\", \"Start Time\" : \"2023-08-02 07:13:16\", \"End Time\": \"2023-08-02 07:26:44\"},\n",
    "    {\"ID_instance\": \"S02_5\", \"Start Time\" : \"2023-08-02 11:06:22\", \"End Time\": \"2023-08-02 11:16:42\"},\n",
    "    {\"ID_instance\": \"S02_6\", \"Start Time\" : \"2023-08-02 12:33:00\", \"End Time\": \"2023-08-02 12:41:28\"},\n",
    "    {\"ID_instance\": \"S02_7\", \"Start Time\" : \"2023-08-02 16:18:34\", \"End Time\": \"2023-08-02 16:28:48\"},\n",
    "    {\"ID_instance\": \"S02_8\", \"Start Time\" : \"2023-08-03 07:09:01\", \"End Time\": \"2023-08-03 07:18:02\"},\n",
    "    {\"ID_instance\": \"S02_9\", \"Start Time\" : \"2023-08-03 16:39:40\", \"End Time\": \"2023-08-03 16:46:27\"},\n",
    "    {\"ID_instance\": \"S02_10\", \"Start Time\": \"2023-08-03 17:21:02\", \"End Time\": \"2023-08-03 17:27:24\"},\n",
    "    {\"ID_instance\": \"S02_11\", \"Start Time\": \"2023-08-04 22:13:08\", \"End Time\": \"2023-08-04 22:24:06\"},\n",
    "    {\"ID_instance\": \"S02_12\", \"Start Time\": \"2023-08-05 05:49:36\", \"End Time\": \"2023-08-05 06:29:37\"},\n",
    "    {\"ID_instance\": \"S02_13\", \"Start Time\": \"2023-08-05 14:05:21\", \"End Time\": \"2023-08-05 14:11:50\"},\n",
    "    {\"ID_instance\": \"S02_14\", \"Start Time\": \"2023-08-05 15:47:28\", \"End Time\": \"2023-08-05 16:17:37\"},\n",
    "    {\"ID_instance\": \"S02_15\", \"Start Time\": \"2023-08-06 08:48:20\", \"End Time\": \"2023-08-06 09:31:08\"},\n",
    "    {\"ID_instance\": \"S02_16\", \"Start Time\": \"2023-08-06 09:48:21\", \"End Time\": \"2023-08-06 09:59:58\"},\n",
    "    {\"ID_instance\": \"S02_17\", \"Start Time\": \"2023-08-07 07:07:35\", \"End Time\": \"2023-08-07 07:15:30\"},\n",
    "    {\"ID_instance\": \"S02_18\", \"Start Time\": \"2023-08-07 12:52:42\", \"End Time\": \"2023-08-07 12:59:34\"},\n",
    "    {\"ID_instance\": \"S02_19\", \"Start Time\": \"2023-08-07 18:21:39\", \"End Time\": \"2023-08-07 18:27:22\"},\n",
    "    {\"ID_instance\": \"S02_20\", \"Start Time\": \"2023-08-08 06:40:41\", \"End Time\": \"2023-08-08 06:50:55\"},\n",
    "    {\"ID_instance\": \"S02_21\", \"Start Time\": \"2023-08-08 10:18:33\", \"End Time\": \"2023-08-08 10:25:43\"},\n",
    "    {\"ID_instance\": \"S02_22\", \"Start Time\": \"2023-08-08 16:43:22\", \"End Time\": \"2023-08-08 16:53:31\"},\n",
    "    {\"ID_instance\": \"S02_23\", \"Start Time\": \"2023-08-09 06:32:09\", \"End Time\": \"2023-08-09 06:39:00\"},\n",
    "    {\"ID_instance\": \"S02_24\", \"Start Time\": \"2023-08-09 16:07:41\", \"End Time\": \"2023-08-09 16:14:22\"},\n",
    "    {\"ID_instance\": \"S02_25\", \"Start Time\": \"2023-08-10 07:19:12\", \"End Time\": \"2023-08-10 07:26:47\"},\n",
    "    {\"ID_instance\": \"S02_26\", \"Start Time\": \"2023-08-10 11:01:34\", \"End Time\": \"2023-08-10 11:12:30\"},\n",
    "    {\"ID_instance\": \"S02_27\", \"Start Time\": \"2023-08-10 12:42:41\", \"End Time\": \"2023-08-10 12:48:16\"},\n",
    "    {\"ID_instance\": \"S02_28\", \"Start Time\": \"2023-08-10 16:52:05\", \"End Time\": \"2023-08-10 17:01:36\"},\n",
    "    {\"ID_instance\": \"S02_29\", \"Start Time\": \"2023-08-11 07:08:23\", \"End Time\": \"2023-08-11 07:15:03\"},\n",
    "    {\"ID_instance\": \"S02_30\", \"Start Time\": \"2023-08-11 12:01:19\", \"End Time\": \"2023-08-11 12:07:27\"},\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "# Convert the commuting instance data to a DataFrame\n",
    "commuting_df = pd.DataFrame(commuting_data)\n",
    "\n",
    "# Convert the Start Time and End Time columns to datetime\n",
    "commuting_df['Start Time'] = pd.to_datetime(commuting_df['Start Time'])\n",
    "commuting_df['End Time'] = pd.to_datetime(commuting_df['End Time'])\n",
    "\n",
    "# Display the commuting instance DataFrame\n",
    "commuting_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Data column in weather data to datetime\n",
    "weather_data['Data'] = pd.to_datetime(weather_data['Data'])\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over each commuting instance\n",
    "for _, row in commuting_df.iterrows():\n",
    "    # Filter the weather data within the date-time range of the commuting instance\n",
    "    mask = (weather_data['Data'] >= row['Start Time']) & (weather_data['Data'] <= row['End Time'])\n",
    "    filtered_data = weather_data[mask]\n",
    "    \n",
    "    # Calculate the average, min, and max for Air Temperature and Relative Humidity \n",
    "    min_temp = filtered_data['Air Temperature Min (\\'C)'].min()\n",
    "    max_temp = filtered_data['Air Temperature Max (\\'C)'].max()\n",
    "    avg_temp = filtered_data['Air Temperature Ave (\\'C)'].mean()\n",
    "\n",
    "    min_rh = filtered_data['Relative Humidity Min (%)'].min()\n",
    "    max_rh = filtered_data['Relative Humidity Max (%)'].max()\n",
    "    avg_rh = filtered_data['Relative Humidity Ave (%)'].mean()\n",
    "    \n",
    "    # min_sr = filtered_data['Solar Radiation Ave (W/m2)'].min()\n",
    "    # max_sr = filtered_data['Solar Radiation Ave (W/m2)'].max()\n",
    "    avg_sr = filtered_data['Solar Radiation Ave (W/m2)'].mean()\n",
    "\n",
    "    # Append the results\n",
    "    results.append({\n",
    "        'ID_instance': row['ID_instance'],\n",
    "        'Min_Air_Temperature': min_temp,\n",
    "        'Max_Air_Temperature': max_temp,\n",
    "        'Avg_Air_Temperature': avg_temp,\n",
    "        'Min_Relative_Humidity': min_rh,\n",
    "        'Max_Relative_Humidity': max_rh,\n",
    "        'Avg_Relative_Humidity': avg_rh,\n",
    "        'Avg_Solar_Radiation': avg_sr\n",
    "    })\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Station Data (Day of Commuting from 8am to 6pm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the provided Excel file\n",
    "file_path = 'C:/Users/Tomar/dev/datasets/weather_summer2023.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Convert the 'Data' column to datetime format\n",
    "data['Data'] = pd.to_datetime(data['Data'])\n",
    "\n",
    "# Define the date ranges as given in the task\n",
    "date_ranges = [\n",
    "    (\"2023-07-31 08:00:00\", \"2023-07-31 18:00:00\"),\n",
    "    (\"2023-08-01 08:00:00\", \"2023-08-01 18:00:00\"),\n",
    "    (\"2023-08-02 08:00:00\", \"2023-08-02 18:00:00\"),\n",
    "    (\"2023-08-03 08:00:00\", \"2023-08-03 18:00:00\"),\n",
    "    (\"2023-08-04 08:00:00\", \"2023-08-04 18:00:00\"),\n",
    "    (\"2023-08-07 08:00:00\", \"2023-08-07 18:00:00\"),\n",
    "    (\"2023-08-08 08:00:00\", \"2023-08-08 18:00:00\"),\n",
    "    (\"2023-08-09 08:00:00\", \"2023-08-09 18:00:00\"),\n",
    "    (\"2023-08-10 08:00:00\", \"2023-08-10 18:00:00\"),\n",
    "    (\"2023-08-11 08:00:00\", \"2023-08-11 18:00:00\")\n",
    "]\n",
    "\n",
    "# Initialize a list to store statistics for each day\n",
    "daily_statistics = []\n",
    "\n",
    "# Calculate statistics for each date range\n",
    "for start, end in date_ranges:\n",
    "    day_data = data[(data['Data'] >= start) & (data['Data'] <= end)]\n",
    "    \n",
    "    stats = {\n",
    "        'Date Range': f'{start} to {end}',\n",
    "        'Min_Air_Temperature': day_data[\"Air Temperature Min ('C)\"].min(),\n",
    "        'Max_Air_Temperature': day_data[\"Air Temperature Max ('C)\"].max(),\n",
    "        'Avg_Air_Temperature': day_data[\"Air Temperature Ave ('C)\"].mean(),\n",
    "        'Min_Relative_Humidity': day_data['Relative Humidity Min (%)'].min(),\n",
    "        'Max_Relative_Humidity': day_data['Relative Humidity Max (%)'].max(),\n",
    "        'Avg_Relative_Humidity': day_data['Relative Humidity Ave (%)'].mean(),\n",
    "        'Avg_Solar_Radiation': day_data['Solar Radiation Ave (W/m2)'].mean()\n",
    "    }\n",
    "    \n",
    "    daily_statistics.append(stats)\n",
    "\n",
    "# Convert the results to a DataFrame for better readability\n",
    "daily_statistics_df = pd.DataFrame(daily_statistics)\n",
    "daily_statistics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Living Lab (Day of Commuting from 8am to 6pm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "file_path = 'C:/Users/Tomar/dev/PCM_study/summer_2023/output/living_lab/LL/LL_b.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure the correct renaming of the 'DateTime' column to 'Timestamp'\n",
    "data.rename(columns={'DateTime': 'Timestamp'}, inplace=True)\n",
    "\n",
    "# Convert the 'Timestamp' column to datetime\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "\n",
    "# Define the start and end times for each day\n",
    "time_intervals = [\n",
    "    (\"2023-07-31 08:00:00\", \"2023-07-31 18:00:00\"),\n",
    "    (\"2023-08-01 08:00:00\", \"2023-08-01 18:00:00\"),\n",
    "    (\"2023-08-02 08:00:00\", \"2023-08-02 18:00:00\"),\n",
    "    (\"2023-08-03 08:00:00\", \"2023-08-03 18:00:00\"),\n",
    "    (\"2023-08-04 08:00:00\", \"2023-08-04 18:00:00\"),\n",
    "    (\"2023-08-07 08:00:00\", \"2023-08-07 18:00:00\"),\n",
    "    (\"2023-08-08 08:00:00\", \"2023-08-08 18:00:00\"),\n",
    "    (\"2023-08-09 08:00:00\", \"2023-08-09 18:00:00\"),\n",
    "    (\"2023-08-10 08:00:00\", \"2023-08-10 18:00:00\"),\n",
    "    (\"2023-08-11 08:00:00\", \"2023-08-11 18:00:00\")\n",
    "    \n",
    "]\n",
    "\n",
    "# Convert string times to datetime\n",
    "time_intervals = [(datetime.strptime(start, \"%Y-%m-%d %H:%M:%S\"), datetime.strptime(end, \"%Y-%m-%d %H:%M:%S\")) for start, end in time_intervals]\n",
    "\n",
    "# Initialize a list to store results\n",
    "results_list = []\n",
    "\n",
    "# Process each time interval\n",
    "for start, end in time_intervals:\n",
    "    # Filter data based on time interval\n",
    "    filtered_data = data[(data['Timestamp'] >= start) & (data['Timestamp'] <= end)]\n",
    "    \n",
    "    # Calculate required metrics\n",
    "    result = {\n",
    "        'Start_Time': start,\n",
    "        'End_Time': end,\n",
    "        'Min_Temp_Air': filtered_data['Temp_Air(C)'].min(),\n",
    "        'Max_Temp_Air': filtered_data['Temp_Air(C)'].max(),\n",
    "        'Avg_Temp_Air': filtered_data['Temp_Air(C)'].mean(),\n",
    "        'Min_Temp_Globe': filtered_data['Temp_Globe(C)'].min(),\n",
    "        'Max_Temp_Globe': filtered_data['Temp_Globe(C)'].max(),\n",
    "        'Avg_Temp_Globe': filtered_data['Temp_Globe(C)'].mean(),\n",
    "        'Min_Relative_Humidity': filtered_data['RH(%)'].min(),\n",
    "        'Max_Relative_Humidity': filtered_data['RH(%)'].max(),\n",
    "        'Mean_Relative_Humidity': filtered_data['RH(%)'].mean(),\n",
    "    }\n",
    "    \n",
    "    # Append result to the list\n",
    "    results_list.append(result)\n",
    "\n",
    "# Create a DataFrame from the results list\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Display the results\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
